<!DOCTYPE html>
<html lang="en">

<body>
<!--https://www.matongxue.com/madocs/607.html/-->
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <!-- Bootstrap CSS -->
    <link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css"
          integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" rel="stylesheet">
    <!-- Customize CSS -->
    <link href="./styles.css" rel="stylesheet"/>
    <!-- Google Fonts -->
    <link href="https://fonts.gstatic.com" rel="preconnect">
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@600&family=Lato:wght@400;700&display=swap"
          rel="stylesheet">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});




    </script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
            type="text/javascript">
    </script>
    <title>Basic Theory Notbook and pointers</title>
</head>
<!-- Navbar -->
<nav class="navbar navbar-expand-lg">
    <div class="container">
        <!--        <a class="navbar-brand" href="#">-->
        <!--            <img src="./assets/logo.svg" class="logo" alt="logo"/>-->
        <!--        </a>-->
        <button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"
                class="navbar-toggler"
                data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" type="button">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                <li class="nav-item">
                    <a aria-current="page" class="nav-link active" href="../index.html">Back</a>
                </li>
            </ul>

        </div>
    </div>
</nav>
<!-- End Navbar -->


<!-- Page Content -->
<div class="container">

    <!-- Intro Section -->
    <section id="intro">
        <div class="row">
            <div class="col-sm intro-text">
                <h1>Basic Theory Notebook and pointers</h1>

            </div>
            <!--            <div class="col-sm">-->
            <!--                <img src="./assets/intro.png" alt="intro" class="img-fluid">-->
            <!--            </div>-->
        </div>
    </section>

    <!-- End Intro Section -->

    <!-- Stay Informed -->
    <section id="details">
        <div class="row">
            <div class="col-sm explanation">
                <img alt="intro" class="img-fluid" src="./assets/seven-books.png">
            </div>
            <div class="col-sm explanation">
                <p> Being a researcher to work on A.I (apologize for using the buzz word), apart from coding skills,
                    one really gotta have to be equipped with a solid background in math and stats.
                    Nobody, maybe except for senior PhDs in math or stats major can claim that
                    they know everything, but at least we need those basic intuitions to understand how an algorithm
                    works, or where the theoretical guarantee comes from.

                    Here are the 7 books I have read. </p>
                <ol>
                    <li>Cassella Book is a must, where you know your bounds, distribution, and where RV comes from.</li>
                    <li>Strang's Linear Algebra doesn't need much of my own explanation</li>
                    <li>This "Green book" is really a good pointer to all the basics.</li>
                    <li>Bishop book is the bible of ML.</li>
                    <li>Boyd book is probably the most practical book out there about optimization. Rockfella one is too
                        hard for me.
                    </li>
                    <li>Statistical Learning book is also very famously fundamental.</li>
                    <li>Goodfellow book is new, but not necessarily the best written one.</li>
                </ol>

                From my experience, we would need the following background at least to understand what's going on:
                <li><a href="https://www.youtube.com/watch?v=7UJ4CFRGd-U&list=PL221E2BBF13BECF6C"> Linear Algebra </a>
                    and its <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">
                        Essence</a></li>
                <li> Probability</li>
                <li> Statistics</li>
                <li><a href="https://www.youtube.com/watch?v=WUvTyaaNkzM"> Calculus (Of course)</a></li>
                As a result, before I go on the job market, there's tons of work for me to keep my memory fresh.


            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2> Expected Value, Variance and Covariance</h2>

                <p> If you claim you know stats, and you flounder at any of the above questions... great
                    embarrassment.
                    <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading7b.pdf">Covariance
                        and Correlation derivation</a>
                <li> Linearity of Expectations $$ \mathbb{E} ( \sum_{j=1}^k c_j g_j(X) )= \sum_{j=1}^k c_j \mathbb{E}
                    (g_j(X))$$
                </li>
                <li> Variance: $\sigma^2={\sf Var}\left(X\right) = \mathbb{E}\left((X-\mu)^2\right)$
                    ${\sf Var}\left(X\right) = \mathbb{E}\left(X^2\right) - \mu^2.$ where $\mu = \mathbb{E}(X)$.<br/>
                    If $X_1, \ldots, X_n$ are independent then: $ {\sf Var}\left(\sum_{i=1}^n a_i X_i\right) = \sum_i
                    a_i^2 {\sf Var}\left(X_i\right). $
                </li>
                <li>The covariance is $${\sf Cov}(X,Y) = \mathbb{E}((X-\mu_x)(Y-\mu_y)) = \mathbb{E}(XY)- \mu_X \mu_Y $$
                    and the correlation is
                    $\rho(X,Y)= {\sf Cov}(X,Y)/\sigma_x\sigma_y$
                    Recall that $-1 \leq \rho(X,Y) \leq 1$
                </li>
                <li>The <b> conditional expectation</b> of $Y$ given $X$
                    is the random variable $\mathbb{E}(Y|X)$ whose value, when $X=x$ is
                    $$
                    \mathbb{E}(Y|X=x) = \int y \ p(y|x)dy
                    $$
                    where
                    $p(y|x) = \frac{p(x,y)}{p(x)}$.
                </li>
                <li><b> Law of Total Expectation or Law of Iterated Expectation </b>:
                    $$
                    \mathbb{E}(Y)= \mathbb{E}\bigl[\mathbb{E}(Y|X)\bigr] = \int \mathbb{E}(Y|X=x) p_X(x)dx.
                    $$
                    The <b>Law of Total Variance</b> is
                    $$
                    {\sf Var}(Y) = {\sf Var}\bigl[ \mathbb{E}(Y|X)\bigr] + \mathbb{E}\bigl[ {\sf Var}(Y|X)\bigr].
                    $$
                </li>
                <li>The sample mean is
                    $$
                    \widehat{\mu}_n = \frac{1}{n}\sum_i X_i
                    $$
                    Why sample variance has a divider of
                    <a href="https://www.willemsleegers.com/posts/why-divide-by-n-1/">(n-1)?</a>
                    $$
                    \widehat{\sigma}^2_n= \frac{1}{n-1} \sum_i (X_i- \widehat{\mu}_n)^2.
                    $$
                    The <strong> sampling distribution</strong> of $\widehat{\mu}_n$ is
                    $$
                    G_n(t) = \mathbb{P}(\widehat{\mu}_n \leq t).
                    $$
                    <a href="https://leetcode.com/problems/random-pick-with-weight/">LeetCode528</a> talks about inverse
                    CDF sampling.
                    $X_i = F^{-1}(U_i).$ $U_1,\cdots,U_n$ independently with distribution U[0,1]
                </li>
                <li>The <b> moment generating function (mgf)</b> is
                    $$
                    M_X(t) = \mathbb{E}\left(e^{tX}\right).
                    $$
                    If $M_X(t)=M_Y(t)$ for all $t$ in an interval around 0 then
                    $X\stackrel{d}{=}Y.$
                    We can take derivatives of the mgf with respect to $t$ and evaluate at $t = 0$, i.e. we have that
                    $ M^{(n)}_X(t)|_{t=0} = \mathbb{E}\left(X^n\right). $

                </li>
            </div>
        </div>
        <div class="row">

            <div class="col-md-12">

                <h2> Common Families of Distributions</h2>

                <li><b>Normal (Gaussian)</b>
                    $X\sim N(\mu,\sigma^2)$ if
                    $$
                    p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-(x-\mu)^2/(2\sigma^2)}.
                    $$
                    If $X \in \mathbb{R}^d$ then
                    $X\sim N(\mu,\Sigma)$ if
                    $$
                    p(x) = \frac{1}{ (2\pi)^{d/2} |\Sigma|} \exp\left( - \frac{1}{2} (x-\mu)^T \Sigma^{-1}
                    (x-\mu)\right).
                    $$
                    Then
                    $\mathbb{E}(Y) = \mu$ and
                    ${\sf cov}(Y) = \Sigma$.
                    The moment generating function is
                    $$
                    M(t) = \exp\left( \mu^T t + \frac{t^T \Sigma t}{2}\right).
                    $$
                </li>

                <li><b>Chi-squared</b>
                    $X\sim \chi^2_p$ if
                    $X = \sum_{j=1}^p Z_j^2$
                    where
                    $Z_1,\ldots, Z_p\sim N(0,1)$.

                    <b>Non-central chi-square.</b>$X \sim \chi^2_1(\mu^2)$ if $X = Z^2$ where $Z \sim N(\mu,1)$.


                </li>
                <li>
                    <b> Bernoulli.</b>
                    $X\sim {\rm Bernoulli}(\theta)$ if
                    $\mathbb{P}(X=1)=\theta$ and
                    $\mathbb{P}(X=0)=1-\theta$ and hence
                    $$
                    p(x)= \theta^x (1-\theta)^{1-x}\ \ \ \ \ \ x=0,1.
                    $$
                </li>
                <li><b> Binomial.</b>
                    $X\sim {\rm Binomial}(\theta)$ if
                    $$
                    p(x)=\mathbb{P}(X=x) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
                    \ \ \ \ \ \ x\in\{-1,\ldots, n\}.
                    $$
                </li>
                <li><b>Poisson</b>
                    $X\sim {\rm Poisson}(\lambda)$ if
                    $P(X=x) = \frac{e^{-\lambda}\lambda^x}{x!}$ $x=0,1,2,\ldots$.
                    The
                    $\mathbb{E}\left(X\right) = {\sf Var}\left(X\right) = \lambda$
                    and
                    $M_X(t) = e^{\lambda({e^t}-1)}$.
                    We can use
                    the mgf to show: if $X_1 \sim {\rm Poisson}(\lambda_1)$, $X_2 \sim {\rm Poisson}(\lambda_2)$,
                    independent then
                    $Y=X_1 + X_2 \sim {\rm Poisson} (\lambda_1 + \lambda_2)$.

                </li>
                <li><b>Exponential</b>
                    $X\sim {\rm exp}(\beta)$ if
                    $p_X(x) = \frac{1}{\beta} e^{-x/\beta}$, $x>0$.
                    Note that ${\rm exp}(\beta)=\Gamma(1,\beta)$.
                </li>
                <li><b>Multinomial</b>
                    The multivariate version of a Binomial is called a Multinomial. Consider
                    drawing a ball from an urn with has balls with $k$ different colors
                    labeled ``color 1, color 2, $\ldots$, color $k$.'' Let $p=(p_1,p_2,\ldots,p_k)$
                    where $\sum_j p_j =1$ and $p_j$ is the probability of drawing color $j$. Draw
                    $n$ balls from the urn
                    (independently and with replacement) and let $X=(X_1,X_2,\ldots,X_k)$ be
                    the count of the number of balls of each color drawn. We say that $X$ has
                    a Multinomial $(n,p)$ distribution. The pdf is
                    $$
                    p(x) = {n \choose {x_1,\ldots,x_k}} p_1^{x_1}\ldots p_k^{x_k}.
                    $$
                </li>
                <li><b> Gamma</b>
                    $X \sim \Gamma(\alpha,\beta)$ if
                    $$
                    p_X(x) = \frac 1{\Gamma(\alpha) \beta^\alpha} x^{\alpha-1} e^{-x/\beta}
                    $$
                    for $x>0$
                    where
                    $\Gamma(\alpha) = \int_0^\infty \frac 1{\beta^\alpha} x^{\alpha-1} e^{-x/\beta} dx$.
                </li>


            </div>
        </div>
        <div class="row">

            <div class="col-md-12">

                <h2> Useful Inequalities</h2>
                <ol>
                    <li><b>Markov Inequality</b>
                        The most elementary tail bound is Markov's inequality, which asserts that for a positive random
                        variable $X \geq 0$,
                        \begin{align*}
                        \mathbb{P}(X \geq t) \leq \frac{\mathbb{E}[X]}{t}.
                        \end{align*}
                    </li>
                    <li><b>Chebyshev Inequality</b>
                        Chebyshev's inequality states that for a random variable $X$, with ${\sf Var}(X) = \sigma^2$:
                        \begin{align*}
                        \mathbb{P}( |X - \mathbb{E}[X]| \geq k \sigma) \leq \frac{1}{k^2}~~~~\forall k \geq 0.
                        \end{align*}
                        Proof: Chebyshev's inequality is an immediate consequence of Markov's inequality.
                        \begin{align*}
                        \mathbb{P}( |X - \mathbb{E}[X]| \geq k \sigma) &= \mathbb{P}( |X - \mathbb{E}[X]|^2 \geq k^2
                        \sigma^2)
                        &\leq \frac{\mathbb{E}(|X - \mathbb{E}[X]|^2)}{k^2 \sigma^2} = \frac{1}{k^2}.
                        \end{align*}

                    </li>
                    <li>Define, $\mu = \mathbb{E}[X]$. For any $t > 0$, we have that,
                        \begin{align*}
                        \mathbb{P}((X - \mu) > u) = \mathbb{P}( \exp(t(X - \mu)) > \exp(t u) ) \leq \frac{
                        \mathbb{E}[\exp(t(X - \mu))]}{\exp(t u)}.
                        \end{align*}
                        Now $t$ is a parameter we can choose to get a tight upper bound, i.e. we can write this bound
                        as:
                        \begin{align*}
                        \mathbb{P}((X - \mu) > u) \leq \inf_{0 \leq t \leq b} \exp (-t(u + \mu))
                        \mathbb{E}[\exp(tX)].
                        \end{align*}
                        This bound is known as <b>Chernoff's bound</b>.
                    </li>
                    <li> Formally, a random variable $X$ with mean $\mu$ is <b>sub-Gaussian</b>(tails decay faster than
                        Gaussian) if there exists a positive number $\sigma$ such that,
                        \begin{align*}
                        \mathbb{E}[\exp(t(X - \mu))] \leq \exp (\sigma^2 t^2/2),
                        \end{align*}
                        for all $t \in \mathbb{R}$. Gaussian random variables with variance $\sigma^2$ satisfy the above
                        condition with equality, so a $\sigma$-sub-Gaussian random variable basically just has an mgf
                        that is dominated by a Gaussian with variance $\sigma$.
                    </li>
                    <li>
                        <b>Jensen's inequality: </b> Jensen's inequality states that for a convex function $g:
                        \mathbb{R} \mapsto \mathbb{R}$ we have that,
                        \begin{align*}
                        \mathbb{E}[g(X)] \geq g(\mathbb{E}[X]).
                        \end{align*}
                        If $g$ is concave then the reverse inequality holds.

                    </li>
                    <li>This in turn yields <b>Hoeffding's</b> bound.
                        Suppose that, $X_1,\ldots, X_n$ are independent identically distribution <b>bounded</b> random
                        variables, with $a \leq X_i \leq b$ for all $i$ then,
                        \begin{align*}
                        \mathbb{P}\left( \left| \frac{1}{n} \sum_{i=1}^n X_i - \mu \right| \geq t \right) \leq 2 \exp
                        \left( - \frac{nt^2}{(b-a)^2} \right).
                        \end{align*}
                        This is a two-sided exponential tail inequality for the averages of bounded random variables.
                    </li>
                    <li><b>Bernstein's inequality</b>
                        suppose we had $X_1,\ldots,X_n$ which were i.i.d from a distribution with mean zero, bounded
                        support $[a,b]$, with variance $\mathbb{E}[(X-\mu)^2] = \sigma^2$. Then,
                        \begin{align*}
                        \mathbb{P}(| \widehat{\mu} - \mu| \geq t) \leq 2 \exp\left( - \frac{nt^2}{2(\sigma^2 + (b-a) t)}
                        \right).
                        \end{align*}
                        Roughly this inequality says with probability at least $1 - \delta$,
                        \begin{align*}
                        |\widehat{\mu} - \mu| \leq 4 \sigma \sqrt{ \frac{\ln (2/\delta)}{n}} + \frac{ 4(b-a)
                        \ln(2/\delta)}{n}.
                        \end{align*}
                    </li>
                    <li><b>The union bound.</b>
                        This is also known as Boole's inequality. It says that if we have events $A_1,\ldots,A_n$ then
                        \begin{align*}
                        \mathbb{P}\left( \bigcup_{i=1}^n A_i\right) \leq \sum_{i=1}^n \mathbb{P}(A_i).
                        \end{align*}
                    </li>
                    <li>Levy's inequality/Tsirelson's inequality: Concentration of Lipschitz functions of Gaussian
                        random variables:
                        \begin{align*}
                        |f(X_1,\ldots,X_n) - f(Y_1,\ldots,Y_n)| \leq L \sqrt{\sum_{i=1}^n (X_i - Y_i)^2},
                        \end{align*}
                        for all $X_1,\ldots,X_n,Y_1,\ldots,Y_n \in \mathbb{R}$.

                        For such functions we have that if $X_1,\ldots,X_n \sim N(0,1)$ then,
                        \begin{align*}
                        \mathbb{P}(|f(X_1,\ldots,X_n) - \mathbb{E}[f(X_1,\ldots,X_n)] | \geq t) \leq 2 \exp \left( -
                        \frac{t^2}{2L^2} \right).
                        \end{align*}
                    </li>
                    <li>A $U$-statistic is defined by a kernel, which is just a function of two random variables, i.e.
                        $g: \mathbb{R}^2 \mapsto \mathbb{R}$. The U-statistic is then given as:
                        \begin{align*}
                        U(X_1,\ldots,X_n) := \frac{1}{ {n \choose 2} } \sum_{j < k} g(X_j, X_k).
                        \end{align*}
                        e.g. Variance, Mean Absolute Deviation.
                    </li>
                    <li>Mcdiarmid's Inequality
                        Formally, we have i.i.d. RVs $X_1,\ldots,X_n$, where each $X_i \in \mathbb{R}$. We have a
                        function $f: \mathbb{R}^n \mapsto \mathbb{R},$ that satisfies the property that:
                        \begin{align*}
                        |f(x_1,\ldots,x_n) - f(x_1,\ldots,x_{k-1}, x'_k, x_{k+1},\ldots,x_n)| \leq L_k,
                        \end{align*}
                        for every $x,x' \in \mathbb{R}^n$, i.e. the function changes by at most $L_k$ if its $k$-th
                        coordinate is changed. This is known as the bounded difference condition.

                        If the random variables $X_1,\ldots, X_n$ are i.i.d then for all $t \geq 0$
                        \begin{align*}
                        \mathbb{P}(|f(X_1,\ldots,X_n) - \mathbb{E}[f(X_1,\ldots,X_n)] | \geq t) \leq 2 \exp \left( -
                        \frac{2t^2}{\sum_{k=1}^n L_k^2} \right).
                        \end{align*}

                        For bounded $U$-statistics, i.e. if $g(X_i, X_j) \leq b$, we can apply Azuma's inequality to
                        obtain a concentration bound. Note that since each random variable $X_i$ participates in $(n-1)$
                        terms we have that,
                        \begin{align*}
                        |U(X_1,\ldots,X_n) - U(X_1,\ldots,X_i',\ldots, X_n)| \leq \frac{1}{{n \choose 2}} (n-1)(2b) =
                        \frac{4b}{n}.
                        \end{align*}
                        So that Azuma's inequality tells us that,
                        \begin{align*}
                        \mathbb{P}(|U(X_1,\ldots,X_n) - \mathbb{E}[U(X_1,\ldots,X_n)] | \geq t) \leq 2 \exp
                        (-nt^2/(8b^2)).
                        \end{align*}
                    </li>
                </ol>
            </div>
        </div>
        <h2> 5 ways to do linear Regression</h2>
        <ol>
            <li><a href="https://mathworld.wolfram.com/LeastSquaresFitting.html">Least Square</a> method and
                it's <a href="https://youtu.be/ewnc1cXJmGA?t=217">derivation</a>
                and <a href="https://www.mathsisfun.com/data/least-squares-regression.html">an intuitive
                    example.</a></li>
            <li>Matrix Formulation for <a href="https://youtu.be/K_EH2abOp00?t=677">Multiple Linear
                Regression</a>, but need to understand
                <a href="https://atmos.washington.edu/~dennis/MatrixCalculus.pdf">Matrix Derivation</a></li>
            <li>Projection Matrix</li>
        </ol>

        There are so much stuff need to review, updating...
        </p>
    </section>
</div>
</body>
</html>