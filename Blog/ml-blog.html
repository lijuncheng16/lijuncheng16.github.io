<!DOCTYPE html>
<html lang="en">
<body>

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <!-- Bootstrap CSS -->
    <link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css"
          integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" rel="stylesheet">
    <!-- Customize CSS -->
    <link href="./styles.css" rel="stylesheet"/>
    <!-- Google Fonts -->
    <link href="https://fonts.gstatic.com" rel="preconnect">
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@600&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@600&family=Lato:wght@400;700&display=swap"
          rel="stylesheet">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});


    </script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
            type="text/javascript">
    </script>
    <title>NeuralNetwork Pointers</title>

    <!--    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>-->

</head>
<nav class="navbar navbar-expand-lg">
    <div class="container">
        <!--        <a class="navbar-brand" href="#">-->
        <!--            <img src="./assets/logo.svg" class="logo" alt="logo"/>-->
        <!--        </a>-->
        <button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"
                class="navbar-toggler"
                data-bs-target="#navbarSupportedContent" data-bs-toggle="collapse" type="button">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                <li class="nav-item">
                    <a aria-current="page" class="nav-link active" href="../index.html">Back</a>
                </li>
            </ul>

        </div>
    </div>
</nav>


<div class="container">

    <!-- Intro Section -->
    <section id="intro">
        <div class="row">
            <div class="col-sm intro-text">
                <h1>Machine Learning pointers</h1>

            </div>
            <!--            <div class="col-sm">-->
            <!--                <img src="./assets/intro.png" alt="intro" class="img-fluid">-->
            <!--            </div>-->
        </div>
    </section>

    <!-- End Intro Section -->

    <!-- Stay Informed -->
    <section id="details">
        <div class="row">

            <div class="col-md-12">
                <h5> Logistic Regression </h5>
                <ol>
                    <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen frameborder="0" height="315"
                            src="https://www.youtube.com/embed/yIYKR4sgzI8?start=472" title="YouTube video player"
                            width="560"></iframe>
                    <li>LR gives us an unconstrained, smooth objective. LR gives calibrated probabilities that can be
                        interpreted as
                        confidence in a decision
                    </li>
                    <li>As an optimization problem, binary class $\ell_2$
                        penalized <a
                                href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">logistic
                            regression</a> minimizes the following cost function:

                        $\min_{w, c} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)$
                        Similarly,
                        $\ell_1$ regularized logistic regression solves the following optimization problem:
                        $\min_{w, c} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1).$
                        Elastic-Net regularization is a combination of and, and minimizes the following cost function:
                        $\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w +
                        c)) + 1),$
                        where $\rho$ controls the strength of $\ell_1$
                        regularization vs. $\ell_2$
                        regularization (it corresponds to the l1_ratio parameter). No regularization amounts to setting
                        C to a very high value..
                    </li>
                </ol>
            </div>
            <div class="col-md-12">
                <h5>SVM</h5>
                <ol>
                    <li>Understanding <a href="https://www.youtube.com/watch?v=gb25CfVZm3Q">RKHS</a> is the basis of
                        understanding Kernels.
                    </li>
                    <li>SVM Primal and Dual derivation.</li>
                    <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen frameborder="0"
                            height="315" src="https://www.youtube.com/embed/_PwhiWxHK8o"
                            title="YouTube video player"
                            width="560"></iframe>
                    <li> SVC solves the following <a
                            href="https://scikit-learn.org/stable/modules/svm.html#svm-classification">primal
                        problem</a>:

                        \begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n}
                        \zeta_i\\\begin{split}\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
                        & \zeta_i \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align}
                        Intuitively, weâ€™re trying to maximize the margin (by minimizing $||w||^2 = w^Tw$), while
                        incurring a penalty when a sample is misclassified or within the margin boundary.
                        Ideally, the value $y_i (w^T \phi (x_i) + b) $would be for all samples, which indicates a
                        perfect prediction.
                        But problems are usually not always perfectly separable with a hyperplane, so we allow some
                        samples to be at a distance $\zeta_i$ from their correct margin boundary.
                        The penalty term C controls the strength of this penalty, and as a result, acts as an inverse
                        regularization parameter (see note below).
                        The dual problem to the primal is:
                        \begin{align}\begin{aligned}\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T
                        \alpha\\\begin{split}
                        \textrm {subject to } & y^T \alpha = 0\\
                        & 0 \leq \alpha_i \leq C, i=1, ..., n\end{split}\end{aligned}\end{align}
                        where $e$ is the vector of all ones, and $Q$ is an n by n positive semidefinite matrix, $Q_{ij}
                        \equiv y_i y_j K(x_i, x_j)$,
                        where $K(x_i, x_j) = \phi (x_i)^T \phi (x_j)$ is the kernel.
                        The terms $\alpha_i$ are called the dual coefficients, and they are upper-bounded by $C$.
                        This dual representation highlights the fact that training vectors are implicitly mapped into a
                        higher
                        (maybe infinite) dimensional space by the <a
                                href="https://www.cs.cmu.edu/~aarti/Class/10701_Spring21/Lecs/svm_dual_kernel_inked.pdf">
                            Kernel Function<a/>. e.g. The <a href="https://youtu.be/Qc5IyLW_hns?t=570">The RBF
                            kernel.</a>
                            Once the optimization problem is solved,
                            the output of decision_function for a given sample x: $\sum_{i\in SV} y_i \alpha_i K(x_i, x)
                            + b$ and the predicted class correspond to its sign.
                            The primal problem can be equivalently formulated as
                            $\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}\max(0, 1 - y_i (w^T \phi(x_i) + b)),$
                            where we make use of the hinge loss. This is the form that is directly optimized by
                            LinearSVC,
                            but unlike the dual form, this one does not involve inner products between samples, so the
                            famous <a href="https://youtu.be/05VABNfa1ds?t=571">kernel trick</a> cannot be applied.

                    </li>
                </ol>

            </div>

        </div>
        <div class="row">

            <div class="col-md-12">
                <h5> Convolutional Networks</h5>
            </div>
            <div class="col-md-12">
                <h5> 3 Resources that covers pretty much all about ConvNets</h5>
                <ol>
                    <li><a href="https://datascience-enthusiast.com/DL/Convolution_model_Step_by_Stepv2.html">Implementation</a>
                        of
                        an example Convolution Network from Strach.
                    </li>
                    <li><a href="https://cs231n.github.io/convolutional-networks/">De facto Convolution
                        resources</a></li>
                    <li><a href="https://youtu.be/8rrHTtUzyZA?t=1292">Wonderful Video</a></li>
                </ol>

                There are so much stuff need to review, updating...
            </div>

        </div>
</body>
</html>


